seed: 64
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

embed_dim: 256
batch_size: 32
encoder_conv_channels: 256
#n_vocab: 33
n_vocab: 128
dropout: 0.05
mel_dim: 80
outputs_per_step: 1
downsample_step: 4
max_target_len: 1024
masked_loss_weight: 0.5
binary_divergence_weight: 0.1
priority_freq: 3000
priority_freq_weight: 0.0
source_sample_rate: 48000
sample_rate: 22050
encoder_in_std_mul: 1.0
encoder_mid_std_mul: 2.0
encoder_out_std_mul: 4.0
encoder_conv_kernel_size: 5
encoder_conv_dilation: 1
encoder_out_kernel_size: 1
preattention_out_channels: 256
preattention_conv_channels: 256
preattention_in_std_mul: 1.0
preattention_conv_std_mul: 2.0
decoder_in_dim: !ref <mel_dim>
decoder_conv_std_mul: 1.0
decoder_conv_channels: 256
decoder_query_position_rate: 1.0
decoder_key_position_rate: 1.29
decoder_max_positions: 512
converter_conv_channels: 256
converter_conv_kernel_size: 5
converter_out_kernel_size: 1
converter_in_std_mul: 1.0
converter_mid_std_mul: 4.0
converter_out_std_mul: 4.0
converter_in_dim: !ref <decoder_conv_channels>
linear_dim: 513
n_fft: 1024
max_mel_len: !ref <decoder_max_positions> // <downsample_step>
max_input_len: 128
max_output_len: 512
hop_length: 256
mel_downsample_step: 4
min_level_db: -100
ref_level_db: 20
pad_linear: !ref <max_output_len>
pad_mel: !ref <max_mel_len>
progress_samples: true
progress_sample_path: samples
progress_samples_interval: 1
overfit_test: true
overfit_test_iterations: 5
guided_attention_sigma: 0.2
train_data_path: ../datasets/testdata/fake_vctk_1ex
valid_data_path: ../datasets/testdata/fake_vctk_1ex


lr: 0.0005
lr_warmup_steps: 4000
number_of_epochs: 50
checkpoint_every_epoch: false

output_folder: !ref ./results/tts/deepvoice/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
tensorboard_logs: !ref <output_folder>/logs


model: !apply:speechbrain.lobes.models.synthesis.deepvoice3ref.deepvoice3
  n_vocab: !ref <n_vocab>
  embed_dim: !ref <embed_dim> 
  mel_dim: !ref <mel_dim> 
  linear_dim: !ref <linear_dim>
  r: !ref <outputs_per_step>
  downsample_step: !ref <downsample_step>
  n_speakers: 1
  speaker_embed_dim: 16
  padding_idx: 0
  dropout: !ref <dropout>
  kernel_size: 5
  encoder_channels: 128
  decoder_channels: 256
  converter_channels: 256
  query_position_rate: 1.0
  key_position_rate: 1.29
  use_memory_mask: False
  trainable_positional_encodings: False
  force_monotonic_attention: True
  use_decoder_state_for_postnet_input: True
  max_positions: 512
  embedding_weight_std: 0.1
  speaker_embedding_weight_std: 0.01
  freeze_embedding: False
  window_ahead: 3
  window_backward: 1
  key_projection: False
  value_projection: False

compute_cost: !new:speechbrain.lobes.models.synthesis.deepvoice3ref.Loss
  linear_dim: !ref <linear_dim>
  downsample_step: !ref <downsample_step>
  outputs_per_step: !ref <outputs_per_step>
  masked_loss_weight: !ref <masked_loss_weight>
  binary_divergence_weight: !ref <binary_divergence_weight>
  priority_freq: !ref <priority_freq>
  priority_freq_weight: !ref <priority_freq_weight>
  sample_rate: !ref <sample_rate>
  guided_attention_sigma: !ref <guided_attention_sigma>

lr_annealing: !new:speechbrain.nnet.schedulers.NoamScheduler
  lr_initial: !ref <lr>
  n_warmup_steps: !ref <lr_warmup_steps>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>
    counter: !ref <epoch_counter>

modules:
  model: !ref <model>

datasets:
  train:
    path: !ref <train_data_path>
  valid:
    path: !ref <valid_data_path>

opt_class: !name:torch.optim.Adam
  lr: !ref <lr>

dataloader_options:
  batch_size: !ref <batch_size>
  shuffle: True  

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>

tensorboard_train_logger: !new:speechbrain.utils.train_logger.TensorboardLogger
  save_dir: !ref <tensorboard_logs>

loggers:
  - !ref <train_logger>
  - !ref <tensorboard_train_logger>

test_frozen_batch: false