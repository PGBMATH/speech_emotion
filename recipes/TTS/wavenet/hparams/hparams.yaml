seed: 42
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

batch_size: 8
dropout: 0.1

# preprocessing pipeline
num_mels: 80
source_sample_rate: 48000
sample_rate: 8000
min_level_db: -100
ref_level_db: 20
highpass_cutoff: 70.0 # remove DC
trim_threshold: -30 # silence trim
silence_threshold: 2 # mulaw domain silence
win_length: 1024
hop_length: 256
n_fft: 1024
power: 1
norm: "slaney"
mel_scale: "slaney"
mel_fmin: 0.0
mel_fmax: 8000.0
mel_normalized: null
max_time_sec: null
max_time_steps: 10240


number_of_epochs: 500
checkpoint_every_epoch: False
checkpoint_frequency: 1

output_folder: !ref ./results/tts/wavenet/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt


progress_samples: True
progress_sample_path: !ref <output_folder>/samples
progress_samples_interval: 10

layers: 50
stacks: 5
residual_channels: 128
gate_channels: 256  # split into 2 gropus internally for gated activation
skip_out_channels: 128
kernel_size: 3
quantize_channels: 256

# Local conditioning (set negative value to disable, e.g. -1))
cin_channels: 80
cin_pad: 2
# global conditioning, -1 for OFF
gin_channels: 1
n_speakers: 1

upsample_params:
  upsample_scales: # product must be equal to hop_length
    - 4
    - 4
    - 4
    - 4
  cin_pad: !ref <cin_pad>
  cin_channels: !ref <cin_channels>

model: !new:speechbrain.lobes.models.synthesis.wavenet.model.WaveNet
  out_channels: !ref <quantize_channels>
  layers: !ref <layers>
  stacks: !ref <stacks>
  residual_channels: !ref <residual_channels>
  gate_channels: !ref <gate_channels>
  skip_out_channels: !ref <skip_out_channels>
  kernel_size: !ref <kernel_size>
  dropout: !ref <dropout>
  cin_channels: !ref <cin_channels>
  cin_pad: !ref <cin_pad>
  upsample_params: !ref <upsample_params>
  gin_channels: !ref <gin_channels>
  n_speakers: !ref <n_speakers>

modules:
  model: !ref <model>

compute_cost: !new:speechbrain.lobes.models.synthesis.wavenet.model.Loss

lr: 0.002
lr_annealing: !new:speechbrain.nnet.schedulers.StepScheduler
  initial_value: !ref <lr>
  decay_factor: 0.98
  decay_drop: 30000

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>
    counter: !ref <epoch_counter>

train_data_path: ../datasets/mockdata/fake_vctk_1ex/
train_filter: null
valid_data_path: ../datasets/mockdata/fake_vctk_1ex/
valid_filter: null

datasets:
    train:
        path: !ref <train_data_path>
        loader: !name:datasets.vctk.load
        filter: !ref <train_filter>
    valid:
        path: !ref <valid_data_path>
        loader: !name:datasets.vctk.load
        filter: !ref <valid_filter>

opt_class: !name:torch.optim.Adam
  lr: !ref <lr>

dataloader_options:
  batch_size: !ref <batch_size>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>
