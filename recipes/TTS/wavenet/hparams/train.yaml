seed: 42
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

embed_dim: 256
batch_size: 32
encoder_conv_channels: 256
dropout: 0.1
mel_dim: 80
outputs_per_step: 1
downsample_step: 4
max_target_len: 1024
masked_loss_weight: 0.5
binary_divergence_weight: 0.1
priority_freq: 3000
priority_freq_weight: 0.0
source_sample_rate: 48000
sample_rate: 22050
n_fft: 400
hop_length: 256
mel_downsample_step: 4
max_positions: 512
max_mel_len: !ref <max_positions> // <downsample_step>

lr: 0.002
lr_warmup_steps: 4000
number_of_epochs: 30

output_folder: !ref ./results/tts/wavenet/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

#model: !new:speechbrain.lobes.models.wavenet.model.WaveNet
model: !new:model.WaveNet

modules:
    model: !ref <model>

# 1. raw [-1, 1]
# 2. mulaw [-1, 1]
# 3. mulaw-quantize [0, mu]
# If input_type is raw or mulaw, network assumes scalar input and discretized mixture 
# of logistic distributions output, otherwise one-hot input and softmax output are assumed.
input_type: "raw"
quantize_channels: 65536  # 65536 or 256

# Model:
# This should equal to `quantize_channels` if mu-law quantize enabled
# otherwise num_mixture * 3 (pi, mean, log_scale)
# single mixture case: 2
num_mixture: 10
out_channels: !ref <num_mixture> * 3
layers: 24
stacks: 4
residual_channels: 128
gate_channels: 256  # split into 2 gropus internally for gated activation
skip_out_channels: 128
kernel_size: 3

# Local conditioning (set negative value to disable, e.g. -1))
cin_channels: -1
cin_pad: 2

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        counter: !ref <epoch_counter>

causal: True
datasets:
  train:
    path: datasets/testdata/fake_vctk_single_speaker

opt_class: !name:torch.optim.Adam
  lr: !ref <lr>

dataloader_options:
  batch_size: !ref <batch_size>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>