seed: 42
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]


sample_rate: 22050



# 1. raw [-1, 1]
# 2. mulaw [-1, 1]
# 3. mulaw-quantize [0, mu]
# If input_type is raw or mulaw, network assumes scalar input and discretized mixture 
# of logistic distributions output, otherwise one-hot input and softmax output are assumed.
input_type: "raw"
quantize_channels: 65536  # 65536 or 256

# Model:
# This should equal to `quantize_channels` if mu-law quantize enabled
# otherwise num_mixture * 3 (pi, mean, log_scale)
# single mixture case: 2
num_mixture: 10
out_channels: !ref <num_mixture> * 3
layers: 24
stacks: 4
residual_channels: 128
gate_channels: 256  # split into 2 gropus internally for gated activation
skip_out_channels: 128
dropout: 0.0
kernel_size: 3

# Local conditioning (set negative value to disable, e.g. -1))
cin_channels: 80
cin_pad: 2
# If True, use transposed convolutions to upsample conditional features,
# otherwise repeat features to adjust time resolution
# (refer to section 2.5 in Wavenet paper)
'''
upsample_conditional_features: True
upsample_net: !new:models.wavenet.upsample.ConvInUpsampleNetwork
    upsample_params={
        "upsample_scales": [4, 4, 4, 4],  # should np.prod(upsample_scales) == hop_size
    },

    # Global conditioning (set negative value to disable)
    # currently limited for speaker embedding
    # this should only be enabled for multi-speaker dataset
    gin_channels=-1,  # i.e., speaker embedding dim
    n_speakers=7,  # 7 for CMU ARCTIC
'''

lr: 0.0005
lr_warmup_steps: 4000
number_of_epochs: 2

model: !new:models.wavenet.model.WavenetModel
# TO ADD

output_folder: !ref ./results/tts/wavenet/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        counter: !ref <epoch_counter>

causal: True
{
  "name": "wavenet_vocoder",
  "input_type": "mulaw-quantize",
  "quantize_channels": 256,
  "preprocess": "",
  "postprocess": "",
  "global_gain_scale": 1.0,
  "sample_rate": 22050,
  "silence_threshold": 2,
  "num_mels": 80,
  "fmin": 125,
  "fmax": 7600,
  "fft_size": 1024,
  "hop_size": 256,
  "frame_shift_ms": null,
  "win_length": 1024,
  "win_length_ms": -1.0,
  "window": "hann",
  "highpass_cutoff": 70.0,
  "output_distribution": "Logistic",
  "log_scale_min": -9.0,
  "out_channels": 256,
  "layers": 2,
  "stacks": 1,
  "residual_channels": 4,
  "gate_channels": 4,
  "skip_out_channels": 4,
  "dropout": 0.0,
  "kernel_size": 3,
  "cin_channels": 80,
  "cin_pad": 2,
  "upsample_conditional_features": true,
  "upsample_net": "ConvInUpsampleNetwork",
  "upsample_params": {
    "upsample_scales": [
      4,
      4,
      4,
      4
    ]
  },
  "gin_channels": -1,
  "n_speakers": 7,
  "pin_memory": true,
  "num_workers": 0,
  "batch_size": 1,
  "optimizer": "Adam",
  "optimizer_params": {
    "lr": 0.001,
    "eps": 1e-08,
    "weight_decay": 0.0
  },
  "lr_schedule": "step_learning_rate_decay",
  "lr_schedule_kwargs": {
    "anneal_rate": 0.5,
    "anneal_interval": 200000
  },
  "max_train_steps": 100,
  "nepochs": 2000,
  "clip_thresh": -1,
  "max_time_sec": null,
  "max_time_steps": 2560,
  "exponential_moving_average": true,
  "ema_decay": 0.9999,
  "checkpoint_interval": 50,
  "train_eval_interval": 50,
  "test_eval_epoch_interval": 1,
  "save_optimizer_state": true
}

datasets:
  train:
    path: datasets/testdata/fake_vctk_single_speaker

opt_class: !name:torch.optim.Adam
  lr: !ref <lr>